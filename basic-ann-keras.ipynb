{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Neural Network Using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required modules and read the \"iris\" dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
       "0   1            5.1           3.5            1.4           0.2  Iris-setosa\n",
       "1   2            4.9           3.0            1.4           0.2  Iris-setosa\n",
       "2   3            4.7           3.2            1.3           0.2  Iris-setosa\n",
       "3   4            4.6           3.1            1.5           0.2  Iris-setosa\n",
       "4   5            5.0           3.6            1.4           0.2  Iris-setosa"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "\n",
    "iris = pd.read_csv(\"data/iris.csv\")\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 10\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>75.500000</td>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.054000</td>\n",
       "      <td>3.758667</td>\n",
       "      <td>1.198667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>43.445368</td>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.433594</td>\n",
       "      <td>1.764420</td>\n",
       "      <td>0.763161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>38.250000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>75.500000</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>112.750000</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm\n",
       "count  150.000000     150.000000    150.000000     150.000000    150.000000\n",
       "mean    75.500000       5.843333      3.054000       3.758667      1.198667\n",
       "std     43.445368       0.828066      0.433594       1.764420      0.763161\n",
       "min      1.000000       4.300000      2.000000       1.000000      0.100000\n",
       "25%     38.250000       5.100000      2.800000       1.600000      0.300000\n",
       "50%     75.500000       5.800000      3.000000       4.350000      1.300000\n",
       "75%    112.750000       6.400000      3.300000       5.100000      1.800000\n",
       "max    150.000000       7.900000      4.400000       6.900000      2.500000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 6)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                 int64\n",
       "SepalLengthCm    float64\n",
       "SepalWidthCm     float64\n",
       "PetalLengthCm    float64\n",
       "PetalWidthCm     float64\n",
       "Species           object\n",
       "dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into input variables (X) and the output class variable (Y). And also split the data into training and test tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = iris.iloc[:,1:5].values\n",
    "y = iris.iloc[:,5].values\n",
    "\n",
    "encoder =  LabelEncoder()\n",
    "y1 = encoder.fit_transform(y)\n",
    "Y = pd.get_dummies(y1).values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a simple Neural Network Model with 1 hidden layer\n",
    "Keras has its layers as a sequence. So, we create a Sequential model and add layers one at a time. As there are 4 input variables, we specify the argument \"input_shape\" as 4. Then we can add as many hidden layers as the problem demands. Since this is a simple datset, I expect to get a good accuracy with just a single hidden layer.<br>\n",
    "\n",
    "We use a fully-connected network, which is defined by \"Dense\" class. The number of neurons in the layer is given as the first argument and specify the activation function using the activation argument.<br>\n",
    "\n",
    "We use the rectifier ('relu') activation function on the first two layers and a sigmoid activation function in the output layer. It seems better performance is achieved using the 'relu' activation function. We use a sigmoid on the output layer so that the output is between 0 and 1.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(10, input_shape=(4,), activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model\n",
    "\n",
    "There are numerical libraries such as Theano or TensorFlow for the compilation of the model I used TensorFlow. There are certain additional parameters that we need to specify, such as, the loss function (to evaluate a set of weights), the optimizer (to search through different weights for the network) and also optional metrics that we like to collect.<br>\n",
    "\n",
    "I used the loss function \"binary_crossentropy\" and the gradient descent algorithm \"adam\". Finally, as it is a classification problem, \"accuracy\" is used as the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(lr=0.04), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the Model\n",
    "\n",
    "This just gives the summary of the model, with parameters in each layer. There are a total of 165 parameters in the eural network model chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 10)                50        \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 8)                 88        \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 165\n",
      "Trainable params: 165\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Model\n",
    "\n",
    "We have defined our model and also compiled it. Noe, we execute the model on the data. We can train our model on the dataset by using fit() function. Here, we specify a parameter called \"epochs\", which are number of iterations that the model needs to undergo. The training process runs for for a fixed number of iterations, here, I specified it to be 100.<br>\n",
    "\n",
    "There areother parameters. such as the \"batch_size\". Whe not specified, it takes the default value of 32. These parameters can be chosen by trial and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 1.0830 - acc: 0.5083\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.7974 - acc: 0.6917\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 0s 58us/step - loss: 0.6402 - acc: 0.7167\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 0s 58us/step - loss: 0.5201 - acc: 0.6917\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 0s 58us/step - loss: 0.4456 - acc: 0.6917\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 0s 67us/step - loss: 0.4224 - acc: 0.7500\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 0s 58us/step - loss: 0.4062 - acc: 0.7250\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 0s 67us/step - loss: 0.3913 - acc: 0.7583\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 0s 58us/step - loss: 0.3619 - acc: 0.8667\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 0s 58us/step - loss: 0.2976 - acc: 0.9333\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 0s 58us/step - loss: 0.2488 - acc: 0.9167\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 0s 58us/step - loss: 0.1945 - acc: 0.9583\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 0s 67us/step - loss: 0.1662 - acc: 0.9417\n",
      "Epoch 14/100\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.1336 - acc: 0.9667\n",
      "Epoch 15/100\n",
      "120/120 [==============================] - 0s 75us/step - loss: 0.1282 - acc: 0.9500\n",
      "Epoch 16/100\n",
      "120/120 [==============================] - 0s 67us/step - loss: 0.1397 - acc: 0.9500\n",
      "Epoch 17/100\n",
      "120/120 [==============================] - 0s 75us/step - loss: 0.1659 - acc: 0.9417\n",
      "Epoch 18/100\n",
      "120/120 [==============================] - 0s 58us/step - loss: 0.2580 - acc: 0.8833\n",
      "Epoch 19/100\n",
      "120/120 [==============================] - 0s 58us/step - loss: 0.1043 - acc: 0.9417\n",
      "Epoch 20/100\n",
      "120/120 [==============================] - 0s 67us/step - loss: 0.1085 - acc: 0.9500\n",
      "Epoch 21/100\n",
      "120/120 [==============================] - 0s 58us/step - loss: 0.2320 - acc: 0.8917\n",
      "Epoch 22/100\n",
      "120/120 [==============================] - 0s 58us/step - loss: 0.1196 - acc: 0.9583\n",
      "Epoch 23/100\n",
      "120/120 [==============================] - 0s 58us/step - loss: 0.1821 - acc: 0.9167\n",
      "Epoch 24/100\n",
      "120/120 [==============================] - 0s 75us/step - loss: 0.1181 - acc: 0.9583\n",
      "Epoch 25/100\n",
      "120/120 [==============================] - 0s 58us/step - loss: 0.1263 - acc: 0.9250\n",
      "Epoch 26/100\n",
      "120/120 [==============================] - 0s 83us/step - loss: 0.1171 - acc: 0.9500\n",
      "Epoch 27/100\n",
      "120/120 [==============================] - 0s 58us/step - loss: 0.0945 - acc: 0.9583\n",
      "Epoch 28/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.0648 - acc: 0.968 - 0s 92us/step - loss: 0.0927 - acc: 0.9667\n",
      "Epoch 29/100\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0959 - acc: 0.9583\n",
      "Epoch 30/100\n",
      "120/120 [==============================] - 0s 67us/step - loss: 0.0791 - acc: 0.9750\n",
      "Epoch 31/100\n",
      "120/120 [==============================] - 0s 75us/step - loss: 0.0878 - acc: 0.9583\n",
      "Epoch 32/100\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0866 - acc: 0.9667\n",
      "Epoch 33/100\n",
      "120/120 [==============================] - 0s 83us/step - loss: 0.0791 - acc: 0.9750\n",
      "Epoch 34/100\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0775 - acc: 0.9750\n",
      "Epoch 35/100\n",
      "120/120 [==============================] - 0s 67us/step - loss: 0.0739 - acc: 0.9833\n",
      "Epoch 36/100\n",
      "120/120 [==============================] - 0s 58us/step - loss: 0.0858 - acc: 0.9667\n",
      "Epoch 37/100\n",
      "120/120 [==============================] - 0s 67us/step - loss: 0.0793 - acc: 0.9750\n",
      "Epoch 38/100\n",
      "120/120 [==============================] - 0s 67us/step - loss: 0.1007 - acc: 0.9500\n",
      "Epoch 39/100\n",
      "120/120 [==============================] - 0s 58us/step - loss: 0.1405 - acc: 0.9583\n",
      "Epoch 40/100\n",
      "120/120 [==============================] - 0s 58us/step - loss: 0.1953 - acc: 0.9167\n",
      "Epoch 41/100\n",
      "120/120 [==============================] - 0s 75us/step - loss: 0.2634 - acc: 0.8917\n",
      "Epoch 42/100\n",
      "120/120 [==============================] - 0s 58us/step - loss: 0.1552 - acc: 0.9417\n",
      "Epoch 43/100\n",
      "120/120 [==============================] - 0s 67us/step - loss: 0.1193 - acc: 0.9500\n",
      "Epoch 44/100\n",
      "120/120 [==============================] - 0s 58us/step - loss: 0.1526 - acc: 0.9417\n",
      "Epoch 45/100\n",
      "120/120 [==============================] - 0s 67us/step - loss: 0.1337 - acc: 0.9500\n",
      "Epoch 46/100\n",
      "120/120 [==============================] - 0s 67us/step - loss: 0.0764 - acc: 0.9583\n",
      "Epoch 47/100\n",
      "120/120 [==============================] - 0s 67us/step - loss: 0.1247 - acc: 0.9500\n",
      "Epoch 48/100\n",
      "120/120 [==============================] - 0s 58us/step - loss: 0.1848 - acc: 0.9417\n",
      "Epoch 49/100\n",
      "120/120 [==============================] - 0s 67us/step - loss: 0.1347 - acc: 0.9333\n",
      "Epoch 50/100\n",
      "120/120 [==============================] - 0s 58us/step - loss: 0.1207 - acc: 0.9583\n",
      "Epoch 51/100\n",
      "120/120 [==============================] - 0s 75us/step - loss: 0.1338 - acc: 0.9583\n",
      "Epoch 52/100\n",
      "120/120 [==============================] - 0s 58us/step - loss: 0.1342 - acc: 0.9500\n",
      "Epoch 53/100\n",
      "120/120 [==============================] - 0s 58us/step - loss: 0.1013 - acc: 0.9583\n",
      "Epoch 54/100\n",
      "120/120 [==============================] - 0s 58us/step - loss: 0.0902 - acc: 0.9583\n",
      "Epoch 55/100\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0779 - acc: 0.9667\n",
      "Epoch 56/100\n",
      "120/120 [==============================] - 0s 67us/step - loss: 0.0837 - acc: 0.9667\n",
      "Epoch 57/100\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0968 - acc: 0.9417\n",
      "Epoch 58/100\n",
      "120/120 [==============================] - 0s 67us/step - loss: 0.1044 - acc: 0.9583\n",
      "Epoch 59/100\n",
      "120/120 [==============================] - 0s 58us/step - loss: 0.0859 - acc: 0.9750\n",
      "Epoch 60/100\n",
      "120/120 [==============================] - 0s 67us/step - loss: 0.0763 - acc: 0.9750\n",
      "Epoch 61/100\n",
      "120/120 [==============================] - 0s 58us/step - loss: 0.0881 - acc: 0.9667\n",
      "Epoch 62/100\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0839 - acc: 0.9667\n",
      "Epoch 63/100\n",
      "120/120 [==============================] - 0s 67us/step - loss: 0.1262 - acc: 0.9583\n",
      "Epoch 64/100\n",
      "120/120 [==============================] - 0s 58us/step - loss: 0.0837 - acc: 0.9750\n",
      "Epoch 65/100\n",
      "120/120 [==============================] - 0s 67us/step - loss: 0.1689 - acc: 0.9250\n",
      "Epoch 66/100\n",
      "120/120 [==============================] - 0s 67us/step - loss: 0.1577 - acc: 0.9583\n",
      "Epoch 67/100\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0919 - acc: 0.9500\n",
      "Epoch 68/100\n",
      "120/120 [==============================] - 0s 58us/step - loss: 0.0779 - acc: 0.9750\n",
      "Epoch 69/100\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0873 - acc: 0.9667\n",
      "Epoch 70/100\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0861 - acc: 0.9667\n",
      "Epoch 71/100\n",
      "120/120 [==============================] - 0s 67us/step - loss: 0.0664 - acc: 0.9750\n",
      "Epoch 72/100\n",
      "120/120 [==============================] - 0s 67us/step - loss: 0.0878 - acc: 0.9500\n",
      "Epoch 73/100\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.1395 - acc: 0.9500\n",
      "Epoch 74/100\n",
      "120/120 [==============================] - 0s 75us/step - loss: 0.1187 - acc: 0.9500\n",
      "Epoch 75/100\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.1263 - acc: 0.9333\n",
      "Epoch 76/100\n",
      "120/120 [==============================] - 0s 67us/step - loss: 0.1000 - acc: 0.9583\n",
      "Epoch 77/100\n",
      "120/120 [==============================] - 0s 58us/step - loss: 0.1179 - acc: 0.9500\n",
      "Epoch 78/100\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0704 - acc: 0.9750\n",
      "Epoch 79/100\n",
      "120/120 [==============================] - 0s 67us/step - loss: 0.1026 - acc: 0.9583\n",
      "Epoch 80/100\n",
      "120/120 [==============================] - 0s 58us/step - loss: 0.0662 - acc: 0.9833\n",
      "Epoch 81/100\n",
      "120/120 [==============================] - 0s 67us/step - loss: 0.0736 - acc: 0.9667\n",
      "Epoch 82/100\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0726 - acc: 0.9750\n",
      "Epoch 83/100\n",
      "120/120 [==============================] - 0s 75us/step - loss: 0.0625 - acc: 0.9833\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 58us/step - loss: 0.0734 - acc: 0.9750\n",
      "Epoch 85/100\n",
      "120/120 [==============================] - 0s 58us/step - loss: 0.0595 - acc: 0.9750\n",
      "Epoch 86/100\n",
      "120/120 [==============================] - 0s 67us/step - loss: 0.0771 - acc: 0.9667\n",
      "Epoch 87/100\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0606 - acc: 0.9833\n",
      "Epoch 88/100\n",
      "120/120 [==============================] - 0s 58us/step - loss: 0.0656 - acc: 0.9750\n",
      "Epoch 89/100\n",
      "120/120 [==============================] - 0s 75us/step - loss: 0.0779 - acc: 0.9583\n",
      "Epoch 90/100\n",
      "120/120 [==============================] - 0s 75us/step - loss: 0.0953 - acc: 0.9500\n",
      "Epoch 91/100\n",
      "120/120 [==============================] - 0s 58us/step - loss: 0.0926 - acc: 0.9500\n",
      "Epoch 92/100\n",
      "120/120 [==============================] - 0s 58us/step - loss: 0.0916 - acc: 0.9667\n",
      "Epoch 93/100\n",
      "120/120 [==============================] - 0s 58us/step - loss: 0.0858 - acc: 0.9667\n",
      "Epoch 94/100\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0789 - acc: 0.9667\n",
      "Epoch 95/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.0541 - acc: 0.968 - 0s 50us/step - loss: 0.0906 - acc: 0.9583\n",
      "Epoch 96/100\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0771 - acc: 0.9667\n",
      "Epoch 97/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.0567 - acc: 0.968 - 0s 58us/step - loss: 0.0627 - acc: 0.9750\n",
      "Epoch 98/100\n",
      "120/120 [==============================] - 0s 58us/step - loss: 0.0735 - acc: 0.9750\n",
      "Epoch 99/100\n",
      "120/120 [==============================] - 0s 67us/step - loss: 0.0595 - acc: 0.9833\n",
      "Epoch 100/100\n",
      "120/120 [==============================] - 0s 67us/step - loss: 0.0583 - acc: 0.9833\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100)\n",
    "\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_class = np.argmax(y_test, axis=1)\n",
    "y_pred_class = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.76730172e-14,   1.86062360e-03,   9.98139381e-01],\n",
       "       [  1.37740353e-04,   9.99671459e-01,   1.90780149e-04],\n",
       "       [  9.99998093e-01,   1.91132995e-06,   3.13139419e-21],\n",
       "       [  2.88519175e-14,   2.14016065e-03,   9.97859895e-01],\n",
       "       [  9.99971032e-01,   2.90222124e-05,   5.85725558e-18],\n",
       "       [  1.16998213e-16,   4.43517871e-04,   9.99556482e-01],\n",
       "       [  9.99981761e-01,   1.82118692e-05,   1.78407266e-18],\n",
       "       [  8.30872523e-07,   9.96442616e-01,   3.55650019e-03],\n",
       "       [  7.51322773e-07,   9.92935836e-01,   7.06345914e-03],\n",
       "       [  4.36771588e-05,   9.99805033e-01,   1.51328422e-04],\n",
       "       [  1.09239987e-11,   1.48574570e-02,   9.85142529e-01],\n",
       "       [  1.65262747e-06,   9.98161614e-01,   1.83677475e-03],\n",
       "       [  1.35341975e-06,   9.90882277e-01,   9.11644660e-03],\n",
       "       [  5.23167216e-07,   9.86147344e-01,   1.38521995e-02],\n",
       "       [  2.92066488e-07,   9.64065552e-01,   3.59341316e-02],\n",
       "       [  9.99951959e-01,   4.80689851e-05,   2.42554336e-17],\n",
       "       [  3.36454434e-07,   9.68009412e-01,   3.19902785e-02],\n",
       "       [  1.31233674e-06,   9.72894788e-01,   2.71039754e-02],\n",
       "       [  9.99905229e-01,   9.47786830e-05,   1.58624350e-16],\n",
       "       [  9.99994159e-01,   5.87374961e-06,   6.66567987e-20],\n",
       "       [  9.64129926e-13,   5.82266459e-03,   9.94177341e-01],\n",
       "       [  1.42109968e-07,   9.03547764e-01,   9.64521691e-02],\n",
       "       [  9.99873161e-01,   1.26786705e-04,   2.57539150e-16],\n",
       "       [  9.99848962e-01,   1.51026674e-04,   7.22971790e-16],\n",
       "       [  1.43184842e-09,   2.47447520e-01,   7.52552509e-01],\n",
       "       [  9.99991059e-01,   8.93189008e-06,   3.61757980e-19],\n",
       "       [  9.99918699e-01,   8.12593207e-05,   4.68866816e-17],\n",
       "       [  1.06519074e-05,   9.99421597e-01,   5.67826035e-04],\n",
       "       [  1.06254453e-03,   9.98892605e-01,   4.48659630e-05],\n",
       "       [  9.99922752e-01,   7.72746644e-05,   6.53193428e-17]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 0, 0, 2, 1, 0,\n",
       "       0, 2, 0, 0, 1, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 0, 0, 2, 1, 0,\n",
       "       0, 2, 0, 0, 1, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of the predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        11\n",
      "          1       1.00      1.00      1.00        13\n",
      "          2       1.00      1.00      1.00         6\n",
      "\n",
      "avg / total       1.00      1.00      1.00        30\n",
      "\n",
      "[[11  0  0]\n",
      " [ 0 13  0]\n",
      " [ 0  0  6]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(classification_report(y_test_class, y_pred_class))\n",
    "print(confusion_matrix(y_test_class, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model\n",
    "\n",
    "We trained the neural network on the train dataset and evaluate its performance on the test dataset. This generates a prediction for each input and output pair and collect scores, including the average loss and any metrics we configured, such as accuracy.<br>\n",
    "\n",
    "We got an accuracy of 100% on the test dataset, which is reiterated from the above evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 0s 2ms/step\n",
      "\n",
      "acc: 100.00%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
